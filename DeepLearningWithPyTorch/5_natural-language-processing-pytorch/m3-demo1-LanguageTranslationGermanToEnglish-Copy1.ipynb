{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this demo we'll be building a machine learning model to go from once sequence to another, using PyTorch and TorchText. This will be done on German to English translations https://www.manythings.org/anki/ (download the data from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we will be representing each word in a language as a one-hot vector.We’ll need a unique index per word to use as the inputs and targets of the networks later.\n",
    "\n",
    "we will use a helper class called Lang which has word → index (word2index) and index → word (index2word) dictionaries, as well as a count of each word word2count to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    \n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    s = ''.join(\n",
    "        char for char in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(char) != 'Mn')\n",
    "    \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To read the data file we will split the file into lines\n",
    "#### - then split lines into pairs and normalize\n",
    "#### - The files are all English → German, to translate German → English add reverse flag to reverse the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse = False):\n",
    "    \n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    lines = open('datasets/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8'). \\\n",
    "                  read().strip().split('\\n')\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\"i am \", \"i m \",\n",
    "                \"he is\", \"he s \",\n",
    "                \"she is\", \"she s \",\n",
    "                \"you are\", \"you re \",\n",
    "                \"we are\", \"we re \",\n",
    "                \"they are\", \"they re \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [p for p in pairs if \n",
    "            len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 176692 sentence pairs\n",
      "Trimmed to 9678 sentence pairs\n",
      "Counted words:\n",
      "deu 4494\n",
      "eng 2913\n",
      "['sie ist ein ziemlich cleveres madchen .', 'she is quite a clever girl .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'deu', True)\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder\n",
    "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decoder(will not be using this in the course)\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string SOS token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Decoder \n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length = MAX_LENGTH):\n",
    "#         super(AttnDecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "#     def forward(self, input, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the SOS token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "<b>“Teacher forcing” </b> is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "       \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  \n",
    "\n",
    "    else:\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize optimizers and criterion\n",
    "- Create set of training pairs\n",
    "- Start empty losses array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = []\n",
    "print_loss_total = 0  \n",
    "plot_loss_total = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.SGD(attn_decoder1.parameters(), lr=0.01)\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                  for i in range(30000)]\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-67c5d7ac2e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     loss = train(input_tensor, target_tensor, encoder1,\n\u001b[0;32m----> 8\u001b[0;31m                  attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d1e0c14ba0a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 40\u001b[0;31m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "for iter in range(1, 30001):\n",
    "    \n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder1,\n",
    "                 attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        print_loss_avg = print_loss_total / 100\n",
    "        print_loss_total = 0\n",
    "        print('iteration - %d loss - %.4f' % (iter, print_loss_avg))\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        plot_loss_avg = plot_loss_total / 100\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "loc = ticker.MultipleLocator(base=0.2)\n",
    "ax.yaxis.set_major_locator(loc)\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEY: > input = target < output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]])  \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
    "                                                                        decoder_hidden, \n",
    "                                                                        encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> wir sind recht stolz auf uns selbst .\n",
      "= we re pretty proud of ourselves .\n",
      "< we re proud of us . <EOS>\n",
      "\n",
      "> du bist brav oder ?\n",
      "= you re good aren t you ?\n",
      "< you re new aren t you ? <EOS>\n",
      "\n",
      "> er ist sehr begierig darauf dorthin zu gehen .\n",
      "= he is very eager to go there .\n",
      "< he is very fond of go . <EOS>\n",
      "\n",
      "> wir gehen .\n",
      "= we re leaving .\n",
      "< we re going . <EOS>\n",
      "\n",
      "> er ist in unserem land sehr bekannt .\n",
      "= he is well known in our country .\n",
      "< he is very good at our new . <EOS>\n",
      "\n",
      "> ich fange allmahlich an es zu bereuen .\n",
      "= i m beginning to regret it .\n",
      "< i am trying to regret it . <EOS>\n",
      "\n",
      "> ich bin opa .\n",
      "= i m a grandfather .\n",
      "< i m a . <EOS>\n",
      "\n",
      "> wir sind seine sohne .\n",
      "= we are his sons .\n",
      "< we are his his . <EOS>\n",
      "\n",
      "> er ist des lesens mude .\n",
      "= he is tired of reading .\n",
      "< he s tired of reading . <EOS>\n",
      "\n",
      "> sie ist viel gro er als ich .\n",
      "= she s way taller than me .\n",
      "< she s much taller than me . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "        \n",
    "        pair = random.choice(pairs)\n",
    "        \n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        \n",
    "        output_words, attentions = evaluate(encoder1, attn_decoder1, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Attention\n",
    "We could simply run plt.matshow(attentions) to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1257afef0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d9d9550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(encoder1, \n",
    "                                    attn_decoder1, \n",
    "                                    'es tut mir sehr leid')\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "plt.matshow(attentions.numpy(), fignum=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate and Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'es tut mir sehr leid' #Re-run for 'es geht mir gut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(encoder1, \n",
    "                                    attn_decoder1, \n",
    "                                    input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = es tut mir sehr leid\n",
      "output = i am very sorry . <EOS>\n"
     ]
    }
   ],
   "source": [
    "print('input =', input_sentence)\n",
    "print('output =', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAAHfCAYAAAC24ow+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XuUZWdZJ+Dfm4SbEC7aYbgFghqUcJGQlovGASUwAbksRkdAHQWRqCMoMqB4QxYwKqjDIANKRBhQRwYYwYxGg6DREQXTMRIggLJAJICjwahcFJL0O3+c01RRVF9CTp2966vnyTorZ++za9fbe3U6/dbvu1R3BwAAYM6Om7oAAACAo9G4AAAAs6dxAQAAZk/jAgAAzJ7GBQAAmD2NCwAAMHsaFwAAYPY0LgAAwOxpXAAAgNnTuAAwmVp4fVXdeepaAJg3jQsAU3pQkv1JvnPqQgCYN40LAFN6fBZNy8Oq6oSpiwFgvjQuAEyiqvYluUt3/26SNyZ55MQlATBjGhcApvJtSX59+f7lWaQvALAtjQsAU3lcFg1LuvuiJLeuqpOnLQmAudK4ALB2VXXzJP+9uz+06fRTk+ybqCQAZq66e+oaAAAAjkjiAsBaVdUTqurU5fuqqpdX1T9X1aVVdfrU9QEwTxoXANbt+5P89fL9Y5LcPckdkzwlyc9PVBMAM6dxAWDdru7uq5bvH5rkld390e5+Y5IbT1gXADOmcQFg3Q5W1a2r6oZJHpDFHi6H3GiimgCYObsUA7Buz0hyIMnxSc7r7ncmSVXdL8n7piwMgPmyqhgAa1dVJyQ5sbuv3HTuxln8f+nj01UGwFxJXIAdVVXHJ7mgu8+auhZm5QuTfG9V3SVJJ7ksyYu7+/9NWxYAc2WOC7CjuvuaJJ+sqptNXQvzUFVfneSi5eErk/zq8v1bl58BwOcwVAzYcVX16iT3SfJ7ST5x6Hx3f99kRTGZqnpLku/p7ku2nL9Hkpd0972nqQyAOTNUDFiH316+IEluurVpSZLu/ouqOnGKggCYP40LsOO6+xVT1zC1qvr3R/q8u39jXbXMQFXVLTZPzF+e/MIYwgzAYWhcYAcsJ6T/dHc/bepa5mA5b+GZSe6QxZ87laS7+4unrGvNHrb89y2TfFWS318ef22SC5Pspcbl+UneUFVPTfLny3NnJHnu8jMA+BwaF9gB3X1NVZ1RVdUmkiXJLyf5gSQXJ7lm4lom0d2PS5Kq+q0kp3X3R5bHt07yoilrW7fuPreqPpzk2Uk2ryr2nO7+P5MWB8BsmZwPO6Sqfi7JqUlek8+ekL6XfrKeJKmqt5pwvVBV7+juu246Pi7JpZvPAQCfy1hiVqqq3nQs5/aIL0zy0SRfl8UwoYcleeikFa1ZVd2zqu6Z5A+q6meq6r6Hzi3P70UXVtUFVfXYqvr2LBYt+IOpi1qn5Spzh94/d8tnb1h/RQDsBhIXVqKqbpjkC7L4C9j9s5jDkCQ3TfI73X3niUpjQlV1pL+Qd3d/3dqKmZHlRP2vWR7+UXe/bsp61q2qLunu05fv/7y777ndZwBcN2effXZfccUVK7/vxRdffEF3n73yGx+FOS6syncleXKS22Rjsm2S/HP22Pj9qvrB7n5eVb0wi7H7n2Uv7V3S3V87dQ1ztBwuuOeGDG5ypJ+Y+WkawIpcccUVOXDgwMrvW1X7Vn7TY6BxWYGq+v4kL0/ysSQvTXJ6kqd3954Z8tDdL0jygqp6Une/cOp6Jvau5b8PxF/CkiRV9W+S/GSS23T3g6vqtCT37e5fnri0tamqP+7uM6vqY/ns3xeHVli76USlTeELqur0LIYr32j5vpavG01aGQCzZajYClTV27r7K6rq3yX53iQ/nuTlm4c/7BVV9W3bne/uV667lqlV1Vcm+ZEkp2TjhwTd3XefrKiJVNXvZNHc/+jyv5UTklzS3XebuDQmcJQhhJI6gBXZv39/X3TRRSu/73HHHXdxd+9f+Y2PQuKyGofmc3x9Fg3L26qqjvQFA/vKTe9vmOQBWQwd23ONS5JfTfK0JG9PcnDiWqa2r7tfXVU/nCTdfXVV7cllkZOkqs5Mcmp3v3wZt5/Y3e+fuq510ZgArM/BgUIKjctqXFxVFyT54iRPr6oTs0f/otrdT9p8XFU3S/IrE5Uztb/v7vOmLmImPlFVX5TlEKmquk+Sf5q2pGlU1U8k2Z/ky7JIoa6fRZP71VPWtW5VdaMkd+rut206d/sk13T3h6arDIC50risxuOT/FiSy7r7k8v/+T554prm4pNJ7jR1ERP5iap6aZI3JfnUoZN7cR+XJE9Jcl6SL6mqNyc5Kck3TlvSZB6ZxTy4P0+S7v7w8ocde83VSX6jqu7e3Yf2OXppFsMrNS4AK9BJRpoWonFZjRdlkbB8XRa7g38syX/NZw+b2hOqanPCcFyS05K8+jCXj+5xSb48yfWykcB19uZqUl+S5MFJTk7yDUnunb3758+nu7ur6lD6dOOpC5pCd19VVa9L8qgkL1v+wOek7l798jcADGGv/sVh1e7d3fesqkuSpLuvrKrrT13URG6VxbyOZPET1b9J8sTpypnUV5h8/hk/3t2vqapbJDkryc8l+YUsGpi95tVV9ZIkN6+qJyT5jiS/NHFNU3lpFr/2lyX5tiyGzgGwMp0eaIFTjctqXFVVx2dj/P5J2aNzXJKc0N1/uPlEVT04yQ9NVM+U3lJVp3X3ZVMXMgOHJuJ/fZJf7O7frKpnTljPZLr7Z6vqgVnscfRlSZ7R3b83cVmT6O53V1Wq6k5JHpPkzKlrAhhKJwfH6Vs0Livy80lel+SWVfVfshi7/2PTlrReVfU9Sf5Tki+uqks3fXRikjdPU9Xkzkzy7VX1/izmuBzar2PPLYec5EPLlOGsJM+tqhtkMZRwT1o2KnuyWdnGL2eRvFza3VdOXQwA82UflxWpqi/PYunfSvKm7n7XUb5kKMvVw26R5KeSPH3TRx/r7n+YpqppVdUdtjvf3R9Ydy1Tq6ovSHJ2krd3919V1a2T3G0vbdK6zcaTn/koe28Dys9Y/t74SJJv6O43Tl0PwEjOOOOMfvNb3rLy+97o+te3j8tu1t3vTvLuqeuYSnf/UxbL2z5m6lrmYi82KIfT3Z/MpkUJuvsjWfxldc/o7r24cthRLX9v3GzqOgCYP40LAAAMqDPWBpR7doz5Tqmqc6auYS48iw2exQbPYoNnscGz2OBZbPAsNngWGzyLvUvjsnr+Y9rgWWzwLDZ4Fhs8iw2exQbPYoNnscGz2OBZXAvdvfLXVAwVAwCAQY20ENdQjcu+ffv6lFNOmbSG29/+9tm/f//kv0MuvvjiqUtIkhzaHRzPYjPPYoNnscGz2OBZbPAsNngWG2byLK7o7pOmLmIvGapxOeWUU3LgwIGpy5iFqpq6BACAkc1+9dDuNjkfAABgnYZKXAAAgA3muAAAALPXGadxMVQMAACYPYkLAAAMqJMcHCdwkbgAAADzJ3EBAIBBmZwPAADMnn1cAAAA1kjiAgAAI+oeaqiYxAUAAJg9iQsAAAyoY3I+AACwC5icDwAAsEYSFwAAGNRIQ8UkLgAAwOxJXAAAYEidjsQFAABgbSQuAAAwoO7k4DiBi8YFAABGZXI+AADAGklcAABgUBIXAACANZK4AADAgDrJwYESF40LAAAMylAxAACANZK4AADAiLqHGiomcQEAAGZv1zQuVfUnU9cAAAC7SXev/DWVXTNUrLu/auoaAABgt+gkHUPF1q6qPj51DQAAwDR2TeJyOFV1TpJzkuT2t7/9xNUAAMB8HBwncNk9icvhdPe53b2/u/efdNJJU5cDAADsgF2fuAAAANuzASUAAMAaSVwAAGBQIyUuu6Zx6e6bTF0DAADsFt2dgwM1LoaKAQAAs7drEhcAAODaGWmomMQFAACYPYkLAAAMaqTEReMCAAAD6sTkfAAAgHWSuAAAwKA6EhcAAIC1kbgAAMCgDo4TuGhcAABgSN1DrSpmqBgAADB7EhcAABhQZ6x9XCQuAADA7ElcAABgUDagBAAAWCOJCwAADGqkOS4aFwAAGNRIjYuhYgAAwOxJXAAAYEDdbXI+AADAOklcAABgUJ1xEheNCwAADOrgOH2LoWIAAMD8SVwAAGBAHcshAwAArJXEBQAABjVS4qJxAQCAQdnHBQAAYI2GSlwuvvjiVNXUZcBsXX3NNVOXMBsnHH/81CXArFX52eYh3QenLgE+P92TDRWrqrOTvCDJ8Ule2t0/veXz2yd5RZKbL695eneff6R7+lMJAABYmao6PsmLkjw4yWlJHlNVp2257MeSvLq7T0/y6CQvPtp9h0pcAACAhQmXQ75Xkvd29/uSpKpeleQRSS7bdE0nueny/c2SfPhoN9W4AAAA18a+qjqw6fjc7j530/Ftk3xw0/HlSe695R7PTPKGqnpSkhsnOeto31TjAgAAg9qhVcWu6O79R/h8u0nnWwt5TJL/0d0/V1X3TfIrVXXXPsKkMo0LAAAMqj+nX1iLy5OcvOn4dvncoWCPT3J2knT3n1bVDZPsS/J3h7upyfkAAMAqXZTk1Kq6Y1VdP4vJ9+dtueZvkjwgSarqzklumOTvj3RTiQsAAAxqirn53X11VT0xyQVZLHX8su5+Z1U9K8mB7j4vyX9O8ktV9QNZDCN7bB9lJQGNCwAAsFLLPVnO33LuGZveX5bkq6/NPTUuAAAwoM6OTc6fhMYFAABG1D3VPi47wuR8AABg9iQuAAAwqJGGiklcAACA2ZO4AADAgDoZao6LxgUAAAY1UuNiqBgAADB7EhcAABiUyfkAAABrJHEBAIAhdToSFwAAgLWRuAAAwIC6F69RaFwAAGBQJucDAACskcQFAAAGZQNKAACANZK4AADAgDpjzXHRuAAAwKAMFQMAAFgjiQsAAIyoW+ICAACwThIXAAAYlcTluquq11fVxVX1zqo6Z3nu41X13OX5N1bVvarqwqp6X1U9fKpaAQBgN+qDvfLXVKYcKvYd3X1Gkv1Jvq+qvijJjZNcuDz/sSTPSfLAJI9M8qztblJV51TVgao6sKa6AQCANZtyqNj3VdUjl+9PTnJqkk8n+d3lubcn+VR3X1VVb09yynY36e5zk5ybJFU1ThYGAADX0UAjxaZpXKrq/knOSnLf7v5kVV2Y5IZJruqNpQ8OJvlUknT3waoyHwcAAPaoqZqBmyW5ctm0fHmS+0xUBwAADKnbBpSr8LtJTqiqS5M8O8lbJqoDAADYBSZJXLr7U0kevM1HN9l0zTO3fM1NPudqAADgsEZKXMwbAQCAIfVQjcuUyyEDAAAcE4kLAAAMasoNI1dN4gIAAMyexAUAAAY02nLIGhcAABjUSI2LoWIAAMDsSVwAAGBUEhcAAID1kbgAAMCgBgpcNC4AADCkbvu4AAAArJPEBQAABmU5ZAAAgDWSuAAAwIA6EhcAAIC1krgAAMCgRkpcNC4AADCokRoXQ8UAAIDZk7gAAMCIuhMbUAIAAKyPxAUAAAY10hwXjQsAAAxqoL7FUDEAAGD+JC4AADCgzlhDxSQuAADA7ElcAABgRD1W4qJxAQCAQfVA+7hoXGAPqaqpSwB2ie6DU5cA8Fk0LgAAMKQeaqiYyfkAAMDsSVwAAGBQEhcAAIA1krgAAMCA2nLIAADArjBQ42KoGAAAMHsSFwAAGNRIWzJJXAAAgNmTuAAAwKBMzgcAAOate6jGxVAxAABg9iQuAAAwKIkLAADAGklcAABgQJ2xEheNCwAAjKiTPjhO42KoGAAAMHsSFwAAGNVAQ8UkLgAAwOxpXAAAYEiLDShX/ToWVXV2Vb2nqt5bVU8/zDXfVFWXVdU7q+p/Hu2ehooBAMCgphgpVlXHJ3lRkgcmuTzJRVV1XndftumaU5P8cJKv7u4rq+qWR7uvxAUAAFileyV5b3e/r7s/neRVSR6x5ZonJHlRd1+ZJN39d0e7qcQFAAAGtUP7uOyrqgObjs/t7nM3Hd82yQc3HV+e5N5b7nGnJKmqNyc5Pskzu/t3j/RNNS4AAMC1cUV37z/C57XNua0d1AlJTk1y/yS3S/J/q+qu3f2Ph7upxgUAAAbU021AeXmSkzcd3y7Jh7e55i3dfVWS91fVe7JoZC463E3NcQEAAFbpoiSnVtUdq+r6SR6d5Lwt17w+ydcmSVXty2Lo2PuOdFOJCwAADGqH5rgc7XteXVVPTHJBFvNXXtbd76yqZyU50N3nLT97UFVdluSaJE/r7o8e6b4aFwAAGNQUjcvy+56f5Pwt556x6X0necrydUwMFQMAAGZP4gIAAEM69p3ud4PZJS61MLu6AACA6exY4lJVz03yge5+8fL4mUk+lkWz9E1JbpDkdd39E1V1SpLfSfIHSe6b5PVVdfPu/oHl1z4hyZ27+5jHwAEAwJ7W081x2Qk7mWy8KsmjNh1/U5K/z2J95nsluUeSM6rq3y4//7Ikr+zu05P8bJKHV9X1lp89LsnLt/smVXVOVR3YsnsnAABwsFf/msiOJS7dfUlV3bKqbpPkpCRXJrl7kgcluWR52U2yaGT+Jot05i3Lr/1EVf1+kodW1buSXK+7336Y73NuknOTpKrGaSkBAIDP2OnJ+a9N8o1JbpVFAnNKkp/q7pdsvmg5VOwTW772pUl+JMm7c5i0BQAA2F4nGWik2I43Lq9K8ktJ9iW5X5K7JXl2Vf1ad3+8qm6b5KrtvrC731pVJye5ZxZJDQAAsEftaOOy3CHzxCQf6u6PJPlIVd05yZ9WVZJ8PMm3ZrFb5nZeneQe3X3lTtYJAAAjGmly/o7v49Ldd9ty/IIkL9jm0rtuc+7MJM/fiboAAGBobR+XHVdVN6+qv0zyL939pqnrAQAAprXjicvno7v/Mcmdpq4DAAB2s55w+eJVm2XiAgAAsNksExcAAOC6M8cFAABgjSQuAAAwoMUGlOMkLhoXAAAY0aJzmbqKlTFUDAAAmD2JCwAADMkGlAAAAGslcQEAgEH1wakrWB2NCwAADMpQMQAAgDWSuAAAwIha4gIAALBWEhcAABjQYv/JcRIXjQsAAAxqpMbFUDEAAGD2JC4AADCkTh+UuAAAAKyNxAUAAEZkOWQAAID1krgAAMCoBkpcNC4AADCogfoWQ8UAAID5k7gAAMCAOibnAwAArJXEBQAARtQZagNKjQvsIccfJ2Q95Py/+IupS5iNp3/bk6cuYTYuvfTCqUsAWKE2VAwAAGCdJC4AADAoiQsAAMAaSVwAAGBQIyUuGhcAABjVQI2LoWIAAMDsSVwAAGBAPdg+LhIXAABg9iQuAAAwqIGmuEhcAACA+ZO4AADAkNpyyAAAwPyN1LgYKgYAAMyexAUAAEbUEhcAAIC1krgAAMCAOmNtQKlxAQCAQRkqBgAAsEYSFwAAGFInEhcAAID1kbgAAMCIBlsOWeMCAACDGqhvMVQMAACYP4kLAAAMaqR9XCQuAADA7ElcAABgQJ2xJudLXAAAgNmTuAAAwIgshwwAAMxfD9W4GCoGAADM3iwSl6o6vruv2XR8QndfPWVNAACw20lcDqOqblxVv11Vb6uqd1TVo6rqAVV1SVW9vapeVlU3WF7711X1jKr64yT/oaourKqfrKo/TPKjVfX+qrre8tqbLq+/3irrBQAAdodVJy5nJ/lwd399klTVzZK8I8kDuvsvq+qVSb4nyX9bXv+v3X3m8trvTnLz7r7f8viUJF+f5PVJHp3kf3f3VVu/YVWdk+ScFf86AABg17MB5eG9PclZVfXcqvqaJKckeX93/+Xy81ck+bebrv9fW75+8/FLkzxu+f5xSV6+3Tfs7nO7e39377+uxQMAwDAWG7ms/jWRlTYuywbljCwamJ9K8oijfMknDnfc3W9OckpV3S/J8d39jlXWCgAA7B4rHSpWVbdJ8g/d/atV9fEk351F8/Gl3f3eJP8xyR9ei1u+MsmvJ3n2KusEAIDRHQpcRrHqOS53S/IzVXUwyVVZzGe5WZLXVNUJSS5K8ovX4n6/luQ5WTQvAADAHrXSxqW7L0hywTYfnb7NtadsOb7/Nl93ZpLXdvc/rqI+AADYS0ZaDnkW+7hsp6pemOTBSR4ydS0AALD79FCNy6pXFVuZ7n5Sd3/pphXJAACAXaCqzq6q91TVe6vq6Ue47hurqqvqqCsEzzZxAQAAroOeZh+Xqjo+yYuSPDDJ5UkuqqrzuvuyLdedmOT7krz1WO4728QFAADYle6V5L3d/b7u/nSSV2X7bVKeneR5Sf71WG6qcQEAgEF198pfSfZV1YFNr3O2fNvbJvngpuPLl+c+o6pOT3Jyd//Wsf5aDBUDAACujSu6+0hzUmqbc58Zs1ZVxyV5fpLHXptvqnEBAIABLTagnGRVscuTnLzp+HZJPrzp+MQkd01yYVUlya2SnFdVD+/uA4e7qcYFAAAGNVHjclGSU6vqjkk+lOTRSb55U03/lGTfoeOqujDJU4/UtCTmuAAAACvU3VcneWIWG9O/K8mru/udVfWsqnr453tfiQsAAAypk4k2oOzu85Ocv+XcMw5z7f2P5Z4SFwAAYPYkLgAAMKJO+uDURayOxgUAAAY10eT8HWGoGAAAMHsSFwAAGJTEBQAAYI0kLgAAMKDOWImLxgUAAEbUYzUuhooBAACzJ3EBAIAhdfqgxAUAAGBtJC4AADAqc1wAAADWR+ICAACD6oyTuGhcAABgQG05ZAAAgPWSuAAAwJA63QenLmJlJC4AAMDsSVyAPekh97jH1CXMxkjjn6+rqpq6BICVGunPeI0LAAAMaqTGxVAxAABg9iQuAAAwKIkLAADAGklcAABgQN1jLYescQEAgFEZKgYAALA+EhcAABhUR+ICAACwNhIXAAAYlOWQAQAA1kjiAgAAgxopcdG4AADAkMbax8VQMQAAYPYkLgAAMKDusYaKSVwAAIDZk7gAAMCgRkpcNC4AADCokRoXQ8UAAIDZk7gAAMCQejFDfxASFwAAYPYkLgAAMKjOOBtQalwAAGBQJucDAACskcQFAAAG1C1xAQAAWCuJCwAADKmHSlw0LgAAMKjucVYVM1QMAACYPYkLAAAMaqShYhIXAABg9iQuAAAwqJESl13fuFTVOUnOmboOAABg5+z6xqW7z01ybpJU1TgtJQAAXBeLHSinrmJldn3jAgAAfK5O0hmncTE5HwAAmL1d07hU1flVdZup6wAAgN2i++DKX1PZNUPFuvshU9cAAABMY9c0LgAAwLXRlkMGAADmb6TGZdfMcQEAAPYuiQsAAAxK4gIAALBGEhcAABhQdyZdvnjVNC4AADCksVYVM1QMAACYPYkLAACMSuICAACwPhIXAAAYVEfiAgAAsDYSFwAAGNRIq4ppXAAAYEg91D4uhooBAACzJ3EBAIABdY81VEziAgAAzJ7EBQAABjVS4qJxAQCAQY3UuBgqBgAAzJ7EBQAABiVxAQAAOIyqOruq3lNV762qp2/z+VOq6rKqurSq3lRVdzjaPTUuAAAwpE764OpfR1FVxyd5UZIHJzktyWOq6rQtl12SZH933z3Ja5M872j31bgAAMCgegf+OQb3SvLe7n5fd386yauSPOKz6ur+g+7+5PLwLUlud7SbalwAAIBrY19VHdj0OmfL57dN8sFNx5cvzx3O45P8ztG+qcn5AAAwoO4dm5x/RXfvP8LntV05215Y9a1J9ie539G+qcYFAABYpcuTnLzp+HZJPrz1oqo6K8mPJrlfd3/qaDfVuAAAwKAmWg75oiSnVtUdk3woyaOTfPPmC6rq9CQvSXJ2d//dsdxU4wKwx1Vtl+jvTX/1t387dQmzcZ+73HPqEmbjox/9yNQlMEvj7I+yat19dVU9MckFSY5P8rLufmdVPSvJge4+L8nPJLlJktcs/z/0N9398CPdV+MCAABD6vQxLF+8I9+5+/wk528594xN78+6tvfUuAAAwKAmGiq2IyyHDAAAzJ7EBQAABiVxAQAAWCOJCwAADGgHN6CchMYFAACG1IvuZRCGigEAALMncQEAgEF1ptnHZSdIXAAAgNmTuAAAwKBMzgcAAGZvpMbFUDEAAGD2JC4AADCklrgAAACsk8QFAAAG1J10Ww4ZAABgbSQuAAAwqJHmuGhcAABgUCM1LoaKAQAAsydxAQCAIfVihv4gJC4AAMDsSVwAAGBQnXESF40LAAAMyj4uAAAAayRxAQCAAXVbDhkAAGCtJC4AADCkHipx0bgAAMCgRmpcrvNQsaq6sKreU1V/sXy9dtNn51TVu5evP6uqMzd99tCquqSq3lZVl1XVd13XWgAAgDF9XolLVV0/yfW6+xPLU9/S3Qe2XPPQJN+V5MzuvqKq7pnk9VV1ryQfTXJuknt19+VVdYMkpyy/7hbdfeXn98sBAAAO2bOJS1Xduap+Lsl7ktzpKJf/UJKndfcVSdLdf57kFUm+N8mJWTRNH11+9qnufs/y6x5VVe+oqqdW1UnXpj4AAGBMR21cqurGVfW4qvrjJC9N8q4kd+/uSzZd9mubhor9zPLcXZJcvOV2B5Lcpbv/Icl5ST5QVb9eVd9SVcclSXf/YpIHJ7lRkj+qqtdW1dmHPt+mvnOq6kBVHdjucwAA2Ku6D678NZVjGSr2kSSXJvnO7n73Ya75nKFih1FJOkm6+zur6m5Jzkry1CQPTPLY5WcfTPLsqnpOkrOT/HIWTdDDt96wu8/NYthZqmqcLAwAAPiMYxkq9o1JPpTkdVX1jKq6wzHe+7IkZ2w5d8/l+SRJd7+9u5+fRdPyDZsvXM6FeXGSFyZ5TZIfPsbvCwAALHagXP1rIkdtXLr7Dd39qCRnJvmnJL9ZVW+sqlOO8qXPS/LcqvqiJKmqe2SRqLy4qm5SVfffdO09knxged2DqurSJM9JcmGS07r7yd39zmvx6wIAgD2tk/QO/DOVY15VrLs/muQFSV6wTEOu2fTxr1XVvyzfX9HdZ3X3eVV12yR/shzC9bEk39rdH6mqE5P8YFW9JMm/JPlElsPEspiw/7Du/sB1+pUBAADD+LyWQ+7uP9v0/v5HuO4XkvzCNuc/luQhh/marRP6AQCAz8OeXQ4ZAABgCp9X4gIAAMzflMsXr5rGBQAAhtSGigEAAKzab1yOAAACEklEQVSTxAUAAAYlcQEAAFgjiQsAAAxosdH9OImLxgUAAAY1UuNiqBgAADB7EhcAABhSJwPt4yJxAQAAZk/iAgAAg+qY4wIAALA2EhcAABjUSKuKaVwAAGBQIzUuhooBAACzJ3EBAIABdXfacsgAAADrI3EBAIBBjTTHReMCAACDGqlxMVQMAACYPYkLAAAMSuICAACwRhIXAAAY1UCJi8YFAACG1OnYxwUAAGBtJC4AADCgbpPzAQAA1mq0xOWKJB+YuIZ9yzrwLDbzLDZ4Fhs8iw2zeBan3upWU5eQzORZzIRnscGz2DCXZ3GHqQs4FiMlLkM1Lt190tQ1VNWB7t4/dR1z4Fls8Cw2eBYbPIsNnsUGz2KDZ7HBs9jgWexdQzUuAADABokLAAAwcz1U42Jy/uqdO3UBM+JZbPAsNngWGzyLDZ7FBs9ig2exwbPY4FnsUTVSFwYAACwcd9xxfcIJ11/5fa+66lMXTzHPSOICAADMnjkuAAAwoNE2oNS4AADAqAZqXAwVAwAAZk/iAgAAQ+p0JC4AAABrI3EBAIBBdR+cuoSV0bgAAMCgRlpVzFAxAABg9iQuAAAwqJESF40LAACM6YIk+3bgvlfswD2PqkbqwgAAgDGZ4wIAAMyexgUAAJg9jQsAADB7GhcAAGD2NC4AAMDsaVwAAIDZ07gAAACzp3EBAABmT+MCAADM3v8H8EdrkzfDBD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11db350f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "\n",
    "ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                   ['<EOS>'], rotation=90)\n",
    "ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
